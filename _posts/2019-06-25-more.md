---
layout: post
title: REVIEW
category : deep learning
---

a. L1-l2의 차이 : https://www.stand-firm-peter.me/2018/09/24/l1l2/
    i. Loss 일 때 : 
    ii. l1  = > 절댓값(|y-pred_y|)
    iii. L2 => mse( (y-pred_y)^2)
    iv. Outlier의 효과를 적당히 무시하기 위해서는 l1 loss사용
    v. 이상치에 민감해야 한다면 l2 loss
    vi. Regulerizer 일 때
    
b. Roc 커브와 auc
    i. Logistic regression에서 임계치로 확인되는 과정.
    ii. TPR(tp/tp+fn) / FPR(fp/fp+tn)
c. Implicit feedback이 뭔지
d. Ranking loss
e. 표준화 할때 적분을 하는데 적분을 어떻게 하는지, 적분이 안될때 어떻게 할건지
    i. Quantile  추정
f. a/b테스팅이 왜 필요한지, 뭐가 다른지
g. Non-linearity(**)
    i. Relu better than Sigmoid- 빠른 학습 속도.- 
h. Just one layer for gradient Vanishing
    i.  representation power를 위해서는 어느 정도 이상의 레이어가 필요함.
