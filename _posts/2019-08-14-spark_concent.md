---
layout: post
title: spark의 기본 개념.
category : [deep learning, dev]
---
## spark basic concepts

- 스파크 어플리케이션은 드라이버(driver) 프로세스와 익스큐터(excutor)프로세스로 구성되어 있음.
    - 드라이버 프로세스는 클러스터 노드 중 하나에서 실행되고 main() 함수를 실행함
    - 익스큐터 프로세스는 드라이버 프로세스가 할당한 작업을 수행

- 스파크는 SparkSession이라 불리는 드라이버 프로세스로 제어됨
    - myRange = spark.range(100).toDF("number")
        - "number" 컬럼에 1~100까지 가지는 DataFrame이 서로 다른 익스큐터에 할당됨

- DataFrame
    - 스파크는 Dataset, DataFrame, SQL테이블, RDD라는 추상화 개념을 가지고 있음
    - Dataset은 자바와 스칼라용으로 파이썬이나 R에서 사용 불가 
    - 모두 분산 데이터 모음을 표현함, 가장 쉽고 효율적인 것은 DataFrame

- 파티션
    - 스파크 내부에서 익스큐터들이 분산 작업을 실행할 수 있도록 파티션이라고 불리는 청크 단위로 데이터를 분할함
    - 파티션은 물리적 분할임, 만약 파티션이 하나라면 작업의 병렬성은 1이됨
    - DataFrame을 사용할 경우 자동으로 처리

- ##### 트랜스포메이션
    - 스파크의 데이터구조는 불변성(immutable)을 가짐, 그래서 한번 생성되면 변경이 불가
    - 변경하기 위해서는 트랜스포메이션을 선언해야 함
        - divisBy2 = myRange.where("number % 2 = 0")
        - 이경우에는 트랜스포메이션을 선언만 한 상태이고 실제로 동작(run)을 하기 위해서는 액션을 선언해야함.
    - 트랜스포메이션은 두가지 유형이 있음
        - 좁은 의존성 (Narrow dependencies) : 트랜스포메이션이 해당 파티션 내부에서만 일어나는 경우(ex, where)
        - 넓은 의존성 (Wide dependencies) :  트랜스포메이션이 다른 파티션을 참고해 일어나는 경우(ex. shuffle)

- ##### 지연 연산(lazy evaluation)
    - 스파크는 트랜스포메이션을 바로 실행하지 않고 실행계획을 만들어 실행함.
    - 이는 전체 데이터 흐름을 최적화 하는 장점을 가지고 있음

- ##### 액션
    - 트랜스포메이션에 대한 수행시 액션 명령을 내려야됨
    - 세가지 유형이 존재
        - 콘솔에서 데이터를 보는 액션
        - 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
        - 출력 데이터소스에 저장하는 액션
    - 예시) divisBy2.count()
        - 1. 좁은 트랜스포메이션을 수행
        - 2. 파티션 별로 레코드 수를 카운트(넓은 트랜스포메이션)
        - 3. 네이티브 객체에 결과를 모음.(액션)
- 종합
    - 예시) 파일 read -> sort -> take
        - csv파일 -> spark.dataframe 객체 : read 메서드 (좁은 트랜스포메이션)
        - spark.dataframe -> sorting : sort 메서드 (넓은 트랜스포메이션)
        - sorted dataframe -> take(3) : take 메서드 (액션)
    - 트랜스포메이션(Transformation)
        - 사용자가 만들어낸 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어냄
        - 새로운 DataFrame을 만들어 내려면 트랜스포메이션을 선언 
    - 액션(Action)
        - 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행함
        - 연산을 시작하거나 사용한 언어에 맞는 데이터 타입으로 변경하려면 액션을 호출
    - spark job의 실행순서를 알고싶다면 explain() 메서드 이용

- 스파크의 구조적 데이터
    - Dataset, DataFrame, SQL테이블과 뷰가 있음
    - 각 구조체에 정의된 메서드들을 스파크가 논리적 실행 계획으로 변환 후, 물리적 실행계획으로 변환함
        - 이 과정은 스파크 내 카탈리스트 옵티마이저가 처리함
    - 해당 과정에서 추가적인 최적화의 가부도 확인함
    - 